{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural networks offer great flexibility and power, but this comes at the cost of many tunable hyperparameters.\n",
    "# Choosing the right values for these hyperparameters can make a significant difference in model performance.\n",
    "# This script explores techniques for hyperparameter tuning using the California Housing dataset.\n",
    "# We'll prepare the data and build a framework for training, validating, and evaluating neural networks,\n",
    "# with the goal of finding an optimal configuration for this specific regression task.\n",
    "\n",
    "# Let's start by importing the California Housing dataset from Scikit-Learn\n",
    "from sklearn.datasets import fetch_california_housing  # Load the California housing dataset\n",
    "from sklearn.model_selection import train_test_split    # Tool to split data into train/validation/test sets\n",
    "from sklearn.preprocessing import StandardScaler        # Tool to scale features to standard normal distribution\n",
    "\n",
    "# Load the dataset (features and target)\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "# Further split training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "# Standardize the features: zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)      # Fit on training data and transform it\n",
    "X_valid = scaler.transform(X_valid)          # Use the same transformation on validation data\n",
    "X_test = scaler.transform(X_test)            # Use the same transformation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f35989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras from TensorFlow to build and train deep learning models\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define a function that builds a customizable neural network model\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=(8,), **kwargs):\n",
    "    # Create a Sequential model (a linear stack of layers)\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Add an input layer with the specified shape\n",
    "    model.add(keras.layers.InputLayer(shape=input_shape))\n",
    "    \n",
    "    # Add the specified number of hidden layers, each with ReLU activation\n",
    "    for layer in range(n_hidden): # for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    \n",
    "    # Add an output layer with a single neuron (for regression)\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    # Use stochastic gradient descent (SGD) optimizer with the specified learning rate\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    # Compile the model with mean squared error loss (appropriate for regression tasks)\n",
    "    # Add metrics=[\"mae\"] to pass on a metric otherwise KerasRegressor throws the following error:\n",
    "    # ValueError: Could not interpret metric identifier: loss\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ff8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# Wrap the Keras model into a Scikit-Learn compatible regressor\n",
    "# This allows you to use Scikit-Learn tools like cross-validation and grid search\n",
    "keras_reg = KerasRegressor(\n",
    "    model=build_model,\n",
    "    model__n_hidden=2,\n",
    "    model__n_neurons=30,\n",
    "    model__learning_rate=0.01,\n",
    "    model__input_shape=(X_train.shape[1],)  # shape should match input data\n",
    "    #metrics=None  # Prevents Scikeras from misinterpreting the loss\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "# - Run for up to 100 epochs\n",
    "# - Use the validation set to monitor overfitting\n",
    "# - Apply early stopping to halt training if validation loss doesn't improve for 10 consecutive epochs\n",
    "keras_reg.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc445ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow import keras\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 2: Build and compile the model within the function\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=0.01, input_shape=(8,), **kwargs):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mean_absolute_error\"])\n",
    "    return model\n",
    "\n",
    "# Step 3: Wrap with KerasRegressor\n",
    "keras_reg = KerasRegressor(\n",
    "    model=build_model,\n",
    "    model__n_hidden=2,\n",
    "    model__n_neurons=30,\n",
    "    model__learning_rate=0.01,\n",
    "    model__input_shape=(X_train.shape[1],),\n",
    ")\n",
    "\n",
    "# Step 4: Fit the model\n",
    "keras_reg.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ca6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test set\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "print(f\"Mean Squared Error on test set: {mse_test:.2}\")\n",
    "\n",
    "# Select the first 3 samples from the test set to simulate new/unseen input data\n",
    "X_new = X_test[:3]\n",
    "\n",
    "# Use the trained model to predict the target values for these new samples\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "print(f\"Predictions for new samples: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# For sampling learning rates from a logarithmic distribution\n",
    "from scipy.stats import reciprocal\n",
    "# Tool to search over hyperparameter space using randomized sampling\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define distributions of hyperparameters to sample from\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],                     # Number of hidden layers to try\n",
    "    \"n_neurons\": np.arange(1, 100),               # Range of neurons per hidden layer (1 to 99)\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2)       # Log-uniform distribution between 0.0003 and 0.03\n",
    "}\n",
    "\n",
    "# Perform randomized hyperparameter search using 3-fold cross-validation\n",
    "rnd_search_cv = RandomizedSearchCV(\n",
    "    estimator=keras_reg,                # The KerasRegressor wrapper\n",
    "    param_distributions=param_distribs,  # Distributions to sample from\n",
    "    n_iter=10,                # Try 10 random combinations\n",
    "    cv=3                      # Use 3-fold cross-validation\n",
    ")\n",
    "\n",
    "# Train the models on the training data using each sampled hyperparameter set\n",
    "rnd_search_cv.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,                                             # Train up to 100 epochs\n",
    "    validation_data=(X_valid, y_valid),                     # Use validation data to monitor overfitting\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)]  # Stop early if validation loss doesn't improve for 10 epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best combination of hyperparameters found during the randomized search\n",
    "best_params = rnd_search_cv.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best hyperparameters found:\", best_params)\n",
    "\n",
    "# Get the best cross-validation score achieved during the randomized search\n",
    "best_score = rnd_search_cv.best_score_\n",
    "\n",
    "# Print the best score (negative mean squared error by default in Scikit-Learn for regressors)\n",
    "print(\"Best cross-validation score (negative MSE):\", best_score)\n",
    "\n",
    "# Retrieve the actual best Keras model from the randomized search\n",
    "model = rnd_search_cv.best_estimator_.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
