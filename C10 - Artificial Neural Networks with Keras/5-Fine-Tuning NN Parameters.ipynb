{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39a429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural networks offer great flexibility and power, but this comes at the cost of many tunable hyperparameters.\n",
    "# Choosing the right values for these hyperparameters can make a significant difference in model performance.\n",
    "# This script explores techniques for hyperparameter tuning using the California Housing dataset.\n",
    "# We'll prepare the data and build a framework for training, validating, and evaluating neural networks,\n",
    "# with the goal of finding an optimal configuration for this specific regression task.\n",
    "\n",
    "# Let's start by importing the California Housing dataset from Scikit-Learn\n",
    "from sklearn.datasets import fetch_california_housing  # Load the California housing dataset\n",
    "from sklearn.model_selection import train_test_split    # Tool to split data into train/validation/test sets\n",
    "from sklearn.preprocessing import StandardScaler        # Tool to scale features to standard normal distribution\n",
    "\n",
    "# Load the dataset (features and target)\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "# Further split training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "# Standardize the features: zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)      # Fit on training data and transform it\n",
    "X_valid = scaler.transform(X_valid)          # Use the same transformation on validation data\n",
    "X_test = scaler.transform(X_test)            # Use the same transformation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f35989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras from TensorFlow to build and train deep learning models\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define a function that builds a customizable neural network model\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=(8,), **kwargs):\n",
    "    # Create a Sequential model (a linear stack of layers)\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Add an input layer with the specified shape\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # Add the specified number of hidden layers, each with ReLU activation\n",
    "    for layer in range(n_hidden): # for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    \n",
    "    # Add an output layer with a single neuron (for regression)\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    # Use stochastic gradient descent (SGD) optimizer with the specified learning rate\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    # Compile the model with mean squared error loss (appropriate for regression tasks)\n",
    "    # Add metrics=[\"mae\"] to pass on a metric otherwise KerasRegressor throws the following error:\n",
    "    # ValueError: Could not interpret metric identifier: loss\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4ff8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 1.0654 - mae: 0.6764 - val_loss: 0.5290 - val_mae: 0.5384\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5014 - mae: 0.5132 - val_loss: 0.4828 - val_mae: 0.5113\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4559 - mae: 0.4875 - val_loss: 0.4481 - val_mae: 0.4907\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4287 - mae: 0.4739 - val_loss: 0.4301 - val_mae: 0.4650\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4312 - mae: 0.4675 - val_loss: 0.4279 - val_mae: 0.4647\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4596 - mae: 0.4674 - val_loss: 0.4143 - val_mae: 0.4614\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4546 - mae: 0.4725 - val_loss: 0.4426 - val_mae: 0.4594\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4025 - mae: 0.4548 - val_loss: 0.4070 - val_mae: 0.4603\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3879 - mae: 0.4461 - val_loss: 0.4017 - val_mae: 0.4592\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3804 - mae: 0.4410 - val_loss: 0.4038 - val_mae: 0.4476\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3756 - mae: 0.4376 - val_loss: 0.3863 - val_mae: 0.4389\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3698 - mae: 0.4335 - val_loss: 0.3788 - val_mae: 0.4354\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3647 - mae: 0.4296 - val_loss: 0.3854 - val_mae: 0.4319\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3614 - mae: 0.4268 - val_loss: 0.3765 - val_mae: 0.4378\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3566 - mae: 0.4231 - val_loss: 0.3810 - val_mae: 0.4504\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3530 - mae: 0.4224 - val_loss: 0.3671 - val_mae: 0.4335\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3492 - mae: 0.4179 - val_loss: 0.3603 - val_mae: 0.4253\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3451 - mae: 0.4147 - val_loss: 0.3614 - val_mae: 0.4238\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3413 - mae: 0.4136 - val_loss: 0.3546 - val_mae: 0.4154\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3385 - mae: 0.4106 - val_loss: 0.3627 - val_mae: 0.4309\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3365 - mae: 0.4085 - val_loss: 0.3546 - val_mae: 0.4165\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3319 - mae: 0.4048 - val_loss: 0.3738 - val_mae: 0.4387\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3300 - mae: 0.4045 - val_loss: 0.3548 - val_mae: 0.4181\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3280 - mae: 0.4018 - val_loss: 0.3604 - val_mae: 0.4094\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3254 - mae: 0.4006 - val_loss: 0.3432 - val_mae: 0.4062\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3256 - mae: 0.4005 - val_loss: 0.3387 - val_mae: 0.4103\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3219 - mae: 0.3965 - val_loss: 0.3424 - val_mae: 0.4186\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3182 - mae: 0.3950 - val_loss: 0.3339 - val_mae: 0.4058\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3155 - mae: 0.3931 - val_loss: 0.3347 - val_mae: 0.4013\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3136 - mae: 0.3909 - val_loss: 0.3362 - val_mae: 0.4074\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3121 - mae: 0.3894 - val_loss: 0.3434 - val_mae: 0.4209\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3125 - mae: 0.3900 - val_loss: 0.3485 - val_mae: 0.4268\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3095 - mae: 0.3867 - val_loss: 0.3281 - val_mae: 0.3869\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3071 - mae: 0.3853 - val_loss: 0.3339 - val_mae: 0.4047\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3065 - mae: 0.3854 - val_loss: 0.3181 - val_mae: 0.3938\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3028 - mae: 0.3831 - val_loss: 0.3188 - val_mae: 0.3887\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3036 - mae: 0.3828 - val_loss: 0.3306 - val_mae: 0.4031\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3019 - mae: 0.3818 - val_loss: 0.3214 - val_mae: 0.3889\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3030 - mae: 0.3820 - val_loss: 0.3192 - val_mae: 0.3820\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3013 - mae: 0.3808 - val_loss: 0.3251 - val_mae: 0.3943\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3013 - mae: 0.3801 - val_loss: 0.3239 - val_mae: 0.3816\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2991 - mae: 0.3797 - val_loss: 0.3465 - val_mae: 0.4290\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3003 - mae: 0.3804 - val_loss: 0.3131 - val_mae: 0.3845\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.2988 - mae: 0.3789 - val_loss: 0.3351 - val_mae: 0.3922\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2976 - mae: 0.3781 - val_loss: 0.3168 - val_mae: 0.3881\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2955 - mae: 0.3777 - val_loss: 0.3145 - val_mae: 0.3905\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2962 - mae: 0.3769 - val_loss: 0.3113 - val_mae: 0.3775\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2953 - mae: 0.3775 - val_loss: 0.3101 - val_mae: 0.3840\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2986 - mae: 0.3790 - val_loss: 0.3119 - val_mae: 0.3923\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2928 - mae: 0.3756 - val_loss: 0.3167 - val_mae: 0.3952\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2940 - mae: 0.3758 - val_loss: 0.3177 - val_mae: 0.3906\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2929 - mae: 0.3760 - val_loss: 0.3168 - val_mae: 0.3983\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2913 - mae: 0.3748 - val_loss: 0.3067 - val_mae: 0.3869\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2899 - mae: 0.3735 - val_loss: 0.3052 - val_mae: 0.3734\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2903 - mae: 0.3721 - val_loss: 0.3279 - val_mae: 0.4083\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2879 - mae: 0.3729 - val_loss: 0.3131 - val_mae: 0.3913\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2891 - mae: 0.3734 - val_loss: 0.3307 - val_mae: 0.4077\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2893 - mae: 0.3728 - val_loss: 0.3213 - val_mae: 0.4100\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2905 - mae: 0.3723 - val_loss: 0.3082 - val_mae: 0.3880\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2937 - mae: 0.3749 - val_loss: 0.3145 - val_mae: 0.3799\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2940 - mae: 0.3758 - val_loss: 0.3070 - val_mae: 0.3886\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2887 - mae: 0.3728 - val_loss: 0.3051 - val_mae: 0.3759\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2929 - mae: 0.3738 - val_loss: 0.3074 - val_mae: 0.3889\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2879 - mae: 0.3717 - val_loss: 0.3087 - val_mae: 0.3735\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2869 - mae: 0.3710 - val_loss: 0.3155 - val_mae: 0.4021\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2860 - mae: 0.3705 - val_loss: 0.3225 - val_mae: 0.3869\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2849 - mae: 0.3698 - val_loss: 0.3196 - val_mae: 0.3920\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2844 - mae: 0.3701 - val_loss: 0.3081 - val_mae: 0.3829\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2836 - mae: 0.3689 - val_loss: 0.3088 - val_mae: 0.3815\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2846 - mae: 0.3690 - val_loss: 0.3306 - val_mae: 0.4134\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2834 - mae: 0.3696 - val_loss: 0.3162 - val_mae: 0.3880\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2835 - mae: 0.3680 - val_loss: 0.3066 - val_mae: 0.3761\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2812 - mae: 0.3677 - val_loss: 0.3028 - val_mae: 0.3814\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2815 - mae: 0.3668 - val_loss: 0.3011 - val_mae: 0.3772\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2796 - mae: 0.3663 - val_loss: 0.3010 - val_mae: 0.3778\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2804 - mae: 0.3660 - val_loss: 0.3222 - val_mae: 0.4032\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2831 - mae: 0.3682 - val_loss: 0.2996 - val_mae: 0.3722\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2792 - mae: 0.3665 - val_loss: 0.3025 - val_mae: 0.3740\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2811 - mae: 0.3672 - val_loss: 0.3037 - val_mae: 0.3715\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2810 - mae: 0.3668 - val_loss: 0.3011 - val_mae: 0.3763\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.2783 - mae: 0.3652 - val_loss: 0.3060 - val_mae: 0.3858\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2802 - mae: 0.3670 - val_loss: 0.2964 - val_mae: 0.3712\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2797 - mae: 0.3677 - val_loss: 0.3000 - val_mae: 0.3748\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2778 - mae: 0.3644 - val_loss: 0.3000 - val_mae: 0.3764\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2765 - mae: 0.3650 - val_loss: 0.3097 - val_mae: 0.3747\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2780 - mae: 0.3663 - val_loss: 0.3395 - val_mae: 0.4203\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2768 - mae: 0.3638 - val_loss: 0.3197 - val_mae: 0.3811\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2752 - mae: 0.3638 - val_loss: 0.2981 - val_mae: 0.3727\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2779 - mae: 0.3657 - val_loss: 0.3005 - val_mae: 0.3797\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2751 - mae: 0.3641 - val_loss: 0.3016 - val_mae: 0.3749\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2748 - mae: 0.3638 - val_loss: 0.3077 - val_mae: 0.3758\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2749 - mae: 0.3630 - val_loss: 0.3023 - val_mae: 0.3776\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2753 - mae: 0.3635 - val_loss: 0.3002 - val_mae: 0.3745\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2792 - mae: 0.3649 - val_loss: 0.2944 - val_mae: 0.3699\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2758 - mae: 0.3632 - val_loss: 0.3052 - val_mae: 0.3782\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2733 - mae: 0.3627 - val_loss: 0.3016 - val_mae: 0.3782\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2733 - mae: 0.3626 - val_loss: 0.3003 - val_mae: 0.3810\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2743 - mae: 0.3630 - val_loss: 0.2961 - val_mae: 0.3685\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2743 - mae: 0.3628 - val_loss: 0.2983 - val_mae: 0.3722\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2736 - mae: 0.3634 - val_loss: 0.2958 - val_mae: 0.3736\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function build_model at 0x00000246708DFEC0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=1\n",
       "\tmodel__n_hidden=2\n",
       "\tmodel__n_neurons=30\n",
       "\tmodel__learning_rate=0.01\n",
       "\tmodel__input_shape=(8,)\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function build_model at 0x00000246708DFEC0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=1\n",
       "\tmodel__n_hidden=2\n",
       "\tmodel__n_neurons=30\n",
       "\tmodel__learning_rate=0.01\n",
       "\tmodel__input_shape=(8,)\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=<function build_model at 0x00000246708DFEC0>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=1\n",
       "\tmodel__n_hidden=2\n",
       "\tmodel__n_neurons=30\n",
       "\tmodel__learning_rate=0.01\n",
       "\tmodel__input_shape=(8,)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# Wrap the Keras model into a Scikit-Learn compatible regressor\n",
    "# This allows you to use Scikit-Learn tools like cross-validation and grid search\n",
    "keras_reg = KerasRegressor(\n",
    "    model=build_model,\n",
    "    model__n_hidden=2,\n",
    "    model__n_neurons=30,\n",
    "    model__learning_rate=0.01,\n",
    "    model__input_shape=(X_train.shape[1],)  # shape should match input data\n",
    "    #metrics=None  # Prevents Scikeras from misinterpreting the loss\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "# - Run for up to 100 epochs\n",
    "# - Use the validation set to monitor overfitting\n",
    "# - Apply early stopping to halt training if validation loss doesn't improve for 10 consecutive epochs\n",
    "keras_reg.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "435ca6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 2ms/step\n",
      "Mean Squared Error on test set: 0.78\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predictions for new samples: [2.4776602 1.3015292 1.0385396]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test set\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "print(f\"Mean Squared Error on test set: {mse_test:.2}\")\n",
    "\n",
    "# Select the first 3 samples from the test set to simulate new/unseen input data\n",
    "X_new = X_test[:3]\n",
    "\n",
    "# Use the trained model to predict the target values for these new samples\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "print(f\"Predictions for new samples: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bf7afd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter learning_rate for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(learning_rate=0.02822382759081754)`\nCheck the list of available parameters with `estimator.get_params().keys()`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     15\u001b[39m rnd_search_cv = RandomizedSearchCV(\n\u001b[32m     16\u001b[39m     estimator=keras_reg,                \u001b[38;5;66;03m# The KerasRegressor wrapper\u001b[39;00m\n\u001b[32m     17\u001b[39m     param_distributions=param_distribs,  \u001b[38;5;66;03m# Distributions to sample from\u001b[39;00m\n\u001b[32m     18\u001b[39m     n_iter=\u001b[32m10\u001b[39m,                \u001b[38;5;66;03m# Try 10 random combinations\u001b[39;00m\n\u001b[32m     19\u001b[39m     cv=\u001b[32m3\u001b[39m                      \u001b[38;5;66;03m# Use 3-fold cross-validation\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Train the models on the training data using each sampled hyperparameter set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mrnd_search_cv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                             \u001b[49m\u001b[38;5;66;43;03m# Train up to 100 epochs\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Use validation data to monitor overfitting\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Stop early if validation loss doesn't improve for 10 epochs\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1145\u001b[39m     estimator._validate_params()\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1148\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1149\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1150\u001b[39m     )\n\u001b[32m   1151\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, groups, **fit_params)\u001b[39m\n\u001b[32m    892\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m    893\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m    894\u001b[39m     )\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m    902\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1809\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1808\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1809\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    838\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    839\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    840\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    841\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    842\u001b[39m         )\n\u001b[32m    843\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m845\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    863\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    864\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    865\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    866\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    867\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     60\u001b[39m config = get_config()\n\u001b[32m     61\u001b[39m iterable_with_config = (\n\u001b[32m     62\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     64\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m     config = {}\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:717\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    710\u001b[39m fit_params = _check_fit_params(X, fit_params, train)\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    713\u001b[39m     \u001b[38;5;66;03m# here we clone the parameters, since sometimes the parameters\u001b[39;00m\n\u001b[32m    714\u001b[39m     \u001b[38;5;66;03m# themselves might be estimators, e.g. when we search over different\u001b[39;00m\n\u001b[32m    715\u001b[39m     \u001b[38;5;66;03m# estimators in a pipeline.\u001b[39;00m\n\u001b[32m    716\u001b[39m     \u001b[38;5;66;03m# ref: https://github.com/scikit-learn/scikit-learn/pull/26786\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     estimator = \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m start_time = time.time()\n\u001b[32m    721\u001b[39m X_train, y_train = _safe_split(estimator, X, y, train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\hands_on_machine_learning\\C10 - Artificial Neural Networks with Keras\\env\\Lib\\site-packages\\scikeras\\wrappers.py:1165\u001b[39m, in \u001b[36mBaseWrapper.set_params\u001b[39m\u001b[34m(self, **params)\u001b[39m\n\u001b[32m   1161\u001b[39m             \u001b[38;5;28msuper\u001b[39m().set_params(**{param: value})\n\u001b[32m   1162\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m   1163\u001b[39m             \u001b[38;5;66;03m# Give a SciKeras specific user message to aid\u001b[39;00m\n\u001b[32m   1164\u001b[39m             \u001b[38;5;66;03m# in moving from the Keras wrappers\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1165\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1166\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for estimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1167\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mThis issue can likely be resolved by setting this parameter\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1168\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m constructor:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1169\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1170\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCheck the list of available parameters with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1171\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m `estimator.get_params().keys()`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1172\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Invalid parameter learning_rate for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(learning_rate=0.02822382759081754)`\nCheck the list of available parameters with `estimator.get_params().keys()`"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# For sampling learning rates from a logarithmic distribution\n",
    "from scipy.stats import reciprocal\n",
    "# Tool to search over hyperparameter space using randomized sampling\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define distributions of hyperparameters to sample from\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],                     # Number of hidden layers to try\n",
    "    \"n_neurons\": np.arange(1, 100),               # Range of neurons per hidden layer (1 to 99)\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2)       # Log-uniform distribution between 0.0003 and 0.03\n",
    "}\n",
    "\n",
    "# Perform randomized hyperparameter search using 3-fold cross-validation\n",
    "rnd_search_cv = RandomizedSearchCV(\n",
    "    estimator=keras_reg,                # The KerasRegressor wrapper\n",
    "    param_distributions=param_distribs,  # Distributions to sample from\n",
    "    n_iter=10,                # Try 10 random combinations\n",
    "    cv=3                      # Use 3-fold cross-validation\n",
    ")\n",
    "\n",
    "# Train the models on the training data using each sampled hyperparameter set\n",
    "rnd_search_cv.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,                                             # Train up to 100 epochs\n",
    "    validation_data=(X_valid, y_valid),                     # Use validation data to monitor overfitting\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)]  # Stop early if validation loss doesn't improve for 10 epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best combination of hyperparameters found during the randomized search\n",
    "best_params = rnd_search_cv.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best hyperparameters found:\", best_params)\n",
    "\n",
    "# Get the best cross-validation score achieved during the randomized search\n",
    "best_score = rnd_search_cv.best_score_\n",
    "\n",
    "# Print the best score (negative mean squared error by default in Scikit-Learn for regressors)\n",
    "print(\"Best cross-validation score (negative MSE):\", best_score)\n",
    "\n",
    "# Retrieve the actual best Keras model from the randomized search\n",
    "model = rnd_search_cv.best_estimator_.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
